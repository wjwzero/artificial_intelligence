## Thinking1: 举一个你之前做过的预测例子（用的什么模型，解决什么问题，比如我用LR模型，对员工离职进行了预测，效果如何... 请分享到课程微信群中）

## Thinking2: XGBoost, LightGBM, CatBoost是三种基于GBDT的实现，三者之间区别是怎样的
- 均是对GBDT的改进算法。不同点在对GBDT所采用的的优化方法
    - XGBoost 
        - 采用损失函数用泰勒展开；
        - 采用贪心算法寻找最佳分割点，既提高计算速度；
        - 支持并行计算。
        - 加入树模型的复杂度作为基础的正则项避免过拟合；
        - 适用于大规模数据，由于算法参数过多且复杂，不使用处理高维特征数据。
    - LightGBM 
        - 通过直方图（Histogram）算法，基于梯度的单边采样算法（Gradient-based One-side Sampling）算法和互斥特征绑定（Exclusive Feature Bunding）算法分别从减少候选分裂点数量，减少样本数量和减少特征数量等方面提高模型的训练和减少内存的使用
    - CatBoost
        - 高效的处理分类特征（categorical features），首先对分类特征做统计，计算某个分类特征（category）出现的频率，然后加上超参数，生成新的数值型特征（numerical features）
        - 同时使用组合类别特征，丰富了特征维度
        - 采用的基模型是对称决策树，算法的参数少、支持分类变量，通过可以防止过拟合

## Thinking3: 你认为，NGBoost对之后的算法会有怎样的影响
    - 一种用于概率预测的自然梯度增强算法，Natural Gradient Boosting
    - 对不确定性进行概率预测在许多领域非常重要，比如天气预报和医学。概率预测是通过模型对整个输出空间进行概率分布的输出，用户量化不确定性
    - GBM（梯度提升机）对许多结构化数据进行预测上取得了成功（比如XGBoost），但是它们无法对真值输出进行概率预测

    